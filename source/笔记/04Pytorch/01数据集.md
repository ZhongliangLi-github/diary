# 数据集

## IterableDataset

当数据量特别大，无法一次性load进内存时，Pytorch里的Dataset就无法胜任了，此时需要使用IterableDataset.

基本用法只需实现`__init__()`，`__iter()__`，`__len()__`，模板如下：

```
from torch.utils.data import IterableDataset, DataLoader

class MyIterableDataset(IterableDataset):
	def __init__(self):
		# 实现初始化代码
		pass
	
	def __iter__(self):
		# 返回一个数据的迭代器
		pass
	
	def __len__(self):
		# 返回数据长度
		pass

mydataset = MyIterableDataset()  # 可迭代对象
mydataloader = DataLoader(mydataset, shuffle=False, batch_size=batch_size, num_workers=num_workers)  # shuffle必须要是False
```

## DataLoader

DataLoader 是 Pytorch 中用于处理模型输入数据的一个工具类，组合了 dataset + sampler，并在数据集上提供单线程和多线程的可迭代对象，在 DataLoader 中有多个参数：

迭代次数 (iteration) = 样本总数(epoch) / 批处理大小(batch_size)

- epoch：所有的训练样本输入到模型中称为一个epoch；
- iteration：一批样本输入到模型中，成为一个Iteration;
- batch_size：批大小，决定一个epoch有多少个Iteration；
- dataset：决定数据从何读取
- shuffle：每一个 epoch 是否为乱序
- num_workers：是否多进程读取数据
- drop_last：当样本不能被 batch_size 整除时，是否最后一批数据丢弃
- pin_memory：如果为 True 将会把数据放到 GPU 上