# 数据集

**IterableDataset**

​	当数据量特别大，无法一次性load进内存时，Pytorch里的Dataset就无法胜任了，此时需要使用IterableDataset.

​	基本用法只需实现`__init__()`，`__iter()__`，`__len()__`，模板如下：

```
from torch.utils.data import IterableDataset, DataLoader

class MyIterableDataset(IterableDataset):
	def __init__(self):
		# 实现初始化代码
		pass
	
	def __iter__(self):
		# 返回一个数据的迭代器
		pass
	
	def __len__(self):
		# 返回数据长度
		pass

mydataset = MyIterableDataset()  # 可迭代对象
mydataloader = DataLoader(mydataset, shuffle=False, batch_size=batch_size, num_workers=num_workers)  # shuffle必须要是False
```

